# AUTOGENERATED! DO NOT EDIT! File to edit: 06_display.ipynb (unless otherwise specified).

__all__ = ['show_video', 'name', 'env', 'p_alpha', 'p_lambda', 'mean', 'sigma', 'max_length', 'experts', 'batch_size',
           'irl_model', 'policy', 'method', 'frames', 'update_scene', 'plot_animation', 'video', 'Writer', 'writer']

# Cell
import matplotlib
import matplotlib.animation as animation
import matplotlib.pyplot as plt
plt.rcParams['axes.labelsize'] = 14
plt.rcParams['xtick.labelsize'] = 12
plt.rcParams['ytick.labelsize'] = 12

import math
import glob
import io
import base64
from IPython.display import HTML
from IPython import display as ipythondisplay

"""
Utility functions to enable video recording of gym environment and displaying it
To enable video, just do "env = wrap_env(env)""
"""

def show_video(dir_name):
  mp4list = glob.glob(f'{dir_name}/*.mp4')
  if len(mp4list) > 0:
    mp4 = mp4list[0]
    video = io.open(mp4, 'r+b').read()
    encoded = base64.b64encode(video)
    ipythondisplay.display(HTML(data='''<video alt="test" autoplay
                loop controls style="height: 400px;">
                <source src="data:video/mp4;base64,{0}" type="video/mp4" />
             </video>'''.format(encoded.decode('ascii'))))
  else:
    print("Could not find video")

# Cell
from .lunarlander import register as lunarlander_register
from .bipedalwalker import register as biepdalwalker_register
from .carracing import register as carracing_register
lunarlander_register()
biepdalwalker_register()
carracing_register()

# Cell
from sandbox.rocky.tf.envs.base import TfEnv
from rllab.envs.gym_env import GymEnv
from rllab.envs.noisy_env import NoisyActionEnv
from .itirl import ITIRL
from .model import ITIRL_Net
from .policy import NoisyControlPolicy

import tensorflow as tf
from rllab.misc import logger
from airl.utils.log_utils import rllab_logdir, load_latest_experts
import numpy as np
from ctypes import *
import pickle
from tqdm import tqdm
import os
import joblib

# for test
import cv2

tf.reset_default_graph()
#env = TfEnv(GymEnv('BipedalWalker-v2', record_video=False, record_log=False))
#env = TfEnv(GymEnv('Pendulum-v0', record_video=True, record_log=True))
#name = 'MountainCarContinuous-v0'
# name = 'Pendulum-v0'
#name='LunarLanderContinuous-v2'
name='LunarLanderStateful-v0'
# name='BipedalWalkerStateful-v0'
# name='CarRacingStateful-v0'
#name = 'CarRacing-v0'
#env = TfEnv(NoisyActionEnv(GymEnv(name, record_video=False, record_log=False), act_noise=1.3))
#vis_env = TfEnv(NoisyActionEnv(GymEnv(name, record_video=True, record_log=True), act_noise=1.3))
env = TfEnv(NoisyActionEnv(GymEnv(name, record_video=False, record_log=False), act_noise=0.4))
# env = TfEnv(GymEnv(name, record_video=False, record_log=False))
# env.type_status="classic"
env.type_status="Box2D"
# env.type_status="CarRacing"

# env2 = TfEnv(GymEnv(name, record_video=False, record_log=False))
# # # env.type_status="classic"
# env2.type_status="Box2D"

print (env.action_space.low, env.action_space.high)

p_alpha = 1.0
p_lambda = 0.1
mean = np.zeros(env.spec.action_space.flat_dim)
sigma = np.eye(env.spec.action_space.flat_dim)*0.1

max_length=90

# load expert data
#experts = load_latest_experts('data/pendulum', n=num_demos)
#experts = load_latest_experts('data/bipedalwalker', n=num_demos)
experts = None

batch_size=200
irl_model = ITIRL_Net(env.spec, expert_trajs=experts, alpha=p_alpha, p_lambda=p_lambda, sigma=sigma, l2_reg=1e-4)
policy = NoisyControlPolicy(env.spec, mean, sigma, p_alpha, p_lambda)
method = ITIRL(env, policy, irl_model=irl_model, batch_size=batch_size, max_path_length=max_length, n_itr=10, discount=0.997)

frames = []

import time
with rllab_logdir(algo=method, dirname='data/lunarlander/lunarlander_itirl_noise0'):
    with tf.Session():
        itr = 299
        method.start_worker(batch_size)
        obs = env.reset()
        init_state = method.get_env_state()
#         method.set_env_state(init_state, env=env2)

        fn = os.path.join(logger.get_snapshot_dir(), f'itr_{itr}.pkl')
        print ("Loading trained model from", fn)
        if not os.path.isfile(fn):
             print (f'{fn} not found')
             sys.exit(-1)
        params = joblib.load(fn)
        init_state = params['init_state']
        obs = params['obs']
        nominal_control=params['nominal_control'][0:max_length, :]
        method.irl_model.set_params(params['irl_params'])
        print ("Loading trained model succeeded!")

        rwd = []
        for t in tqdm(range(300)):
#             print ('time step', t)
#             method.set_env_state(init_state, env=env2)
#             method.set_env_state(init_state)
#             viewer = env.wrapped_env.env.env.viewer
#             score_label = None
#             if viewer:
#                 score_label = env.wrapped_env.env.env.score_label
#             env.wrapped_env.env.env = pickle.loads(init_state)
#             env.wrapped_env.env.env.viewer = viewer
#             env.wrapped_env.env.env.score_label = score_label
            #for itr in range(2):
#             time.sleep(5)
#             print ("before sample")
            new_control = method.sampler.fast_sampling_and_processing(t, init_state, obs, with_true_weights=False, is_training=False) + policy.nominal_control
#             time.sleep(5)
#             print ("after sample")
            if not method.sampler.good_itr and t > 400:
                break
            policy.update_nominal_control(new_control)
#             check whether set state works
#             print(env.wrapped_env.env.unwrapped.lander.position.tuple, env2.wrapped_env.env.unwrapped.lander.position.tuple)
#             print (env.wrapped_env.env.unwrapped)
#             env.render()
#             env2.render()

            action = policy.get_current_control()
#             action = [0, 0, 0]
#             time.sleep(5)
            obs, reward, done, info = env.step(action)
#             cv2.imshow("obs", obs)
#             cv2.waitKey(1)
#             print ('state', env.wrapped_env.env.get_state())
#             print ('action', action, 'reward', reward, 'done', done)
            rwd.append(reward)
            print ('total so far', np.array(rwd).sum())
            frames.append(env.wrapped_env.env.render('rgb_array'))
            #print ('obs', obs, obs2)
            env.render()
            #update state
            init_state = method.get_env_state()
            policy.elapse_one_step()
            if done:
                break
        env.terminate()
        method.shutdown_worker()

# Cell
def update_scene(num, frames, patch):
    patch.set_data(frames[num])
    return patch,

def plot_animation(frames, repeat=False, interval=40):
    plt.close()  # or else nbagg sometimes plots in the previous cell
    fig = plt.figure()
    patch = plt.imshow(frames[0])
    plt.axis('off')
    return animation.FuncAnimation(fig, update_scene, fargs=(frames, patch), frames=len(frames), repeat=repeat, interval=interval)
video = plot_animation(frames)
# Set up formatting for the movie files
Writer = animation.writers['ffmpeg']
writer = Writer(fps=15, metadata=dict(artist='Me'), bitrate=1800)
video.save('videos/lunarlander_learnednoise0_itirl_noise0.4.mp4', writer=writer)
plt.show()
#show_video('data/test/gym_log')
