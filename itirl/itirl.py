# AUTOGENERATED! DO NOT EDIT! File to edit: 03_ITIRL.ipynb (unless otherwise specified).

__all__ = ['ITIRL']

# Cell
from rllab.algos.base import RLAlgorithm
import rllab.misc.logger as logger
import rllab.plotter as plotter
import tensorflow as tf
#from sandbox.rocky.tf.samplers.batch_sampler import BatchSampler
#from sandbox.rocky.tf.samplers.vectorized_sampler import VectorizedSampler
from .itvecsampler import ITVectorizedSampler
import numpy as np
import time
from collections import deque
import joblib
import pickle
import os
import sys
from ctypes import *

from airl.utils.hyperparametrized import Hyperparametrized
from airl.utils.log_utils import get_expert_fnames
import cv2

# Cell
class ITIRL(RLAlgorithm, metaclass=Hyperparametrized):
    """
    ITIRL batch parallel sampling method
    # batch_size: number of samples per iteration
    """
    def __init__(
            self,
            env,
            policy,
            scope=None,
            lr=3e-4,
            n_itr=500,
            start_itr=0,
            discount=1.0,
            batch_size=5000,
            eval_batch_size=100,
            max_path_length=500,
            plot=False,
            pause_for_plot=False,
            store_paths=True,
            whole_paths=True,
            fixed_horizon=False,
            sampler_cls=None,
            sampler_args=None,
            force_batch_sampler=False,
            init_pol_params=None,
            irl_model=None,
            irl_model_wt=1.0,
            init_irl_params=None,
            zero_environment_reward=False,
            train_irl=True,
            expert_datadir=None,
            num_demos=5,
            **kwargs
            ):
        self.env = env
        self.policy = policy
        self.scope = scope
        self.lr=lr
        self.n_itr = n_itr
        self.start_itr = start_itr
        self.discount = discount
        self.batch_size = batch_size
        self.eval_batch_size = eval_batch_size
        self.max_path_length = max_path_length
        self.plot = plot
        self.pause_for_plot = pause_for_plot
        self.store_paths = store_paths
        self.whole_paths = whole_paths
        self.fixed_horizon = fixed_horizon
        self.init_pol_params = init_pol_params
        self.init_irl_params = init_irl_params
        self.irl_model = irl_model
        self.irl_model_wt = irl_model_wt
        self.no_reward = zero_environment_reward
        self.train_irl = train_irl
        self.__irl_params = None
        self.expert_datadir = expert_datadir
        self.num_demos = num_demos

        # iniitialize
        if self.irl_model_wt > 0:
           assert self.irl_model is not None, "Need to specify a IRL model"

        if sampler_cls is None:
            if self.policy.vectorized and not force_batch_sampler:
                print ("using vectorized sampler")
                sampler_cls = ITVectorizedSampler
            else:
                print ("Current we only support vectorized sampler")
                exit(-1)

        if sampler_args is None:
            sampler_args = dict()
        self.sampler = sampler_cls(self, **sampler_args)

    def start_worker(self, n_parallel=8):
        self.sampler.start_worker(n_parallel)
        # update the policy info
        self.policy.init_info(self.batch_size, self.max_path_length,                    self.env.spec.action_space.flat_dim, self.n_itr)
        if self.plot:
            plotter.init_worker()
            plotter.init_plot(self.env, self.policy)

    def shutdown_worker(self):
        self.sampler.shutdown_worker()

    def obtain_samples(self, itr):
        return self.sampler.obtain_samples(itr)

    def process_samples(self, itr, paths):
        return self.sampler.process_samples(itr, paths)

    def log_avg_returns(self, paths):
        undiscounted_returns = [sum(path["rewards"]) for path in paths]
        avg_return = np.mean(undiscounted_returns)
        return avg_return

    def get_irl_params(self):
        return self.__irl_params

    def get_env_state(self, env=None):
        if env is None:
            env = self.env
        if env.type_status == "classic":
            # older version has one more layer of wrapper.
            #state = env.wrapped_env.wrapped_env.env.state
            state = env.wrapped_env.env.env.state
        elif env.type_status == "Box2D" or env.type_status == "CarRacing":
            # older version has one more layer of wrapper.
            # env = env.wrapped_env.wrapped_env.env
            env = env.wrapped_env.env.env
            state = pickle.dumps(env)
        else:
            env = env.wrapped_env.env.env
            state = env.unwrapped.sim.get_state()
        return state

    def set_env_state(self, new_state, env=None):
        if env is None:
            env = self.env
        #print ('status', env.type_status)
        if env.type_status == "classic":
            #state = env.wrapped_env.wrapped_env.env.state
            state = env.wrapped_env.env.env.state
            state_size = new_state.dtype.itemsize * new_state.size
            memmove(state.ctypes.data, new_state.ctypes.data, state_size)
        elif env.type_status == 'Box2D' or env.type_status == "CarRacing":
            #env.wrapped_env.wrapped_env.env = pickle.loads(new_state)
            # retain the viewer pointer
            viewer = env.wrapped_env.env.env.viewer
            score_label = None
            transform=None
            if viewer:
                score_label = env.wrapped_env.env.env.score_label
                transform = env.wrapped_env.env.env.transform
            env.wrapped_env.env.env = pickle.loads(new_state)
            env.wrapped_env.env.env.viewer = viewer
            env.wrapped_env.env.env.score_label = score_label
            env.wrapped_env.env.env.transform = transform
        else:
            env = env.wrapped_env.env
            env.unwrapped.sim.set_state(new_state)

    def itr_eval(self, sess, itr, env=None, is_training=True, with_true_weights=False, num_steps=100, num_trials=10):
        if env is None:
            env = self.env
        if is_training:
            fn = os.path.join(logger.get_snapshot_dir(), f'itr_{itr}.pkl')
            print ("Loading trained model from", fn)
            if not os.path.isfile(fn):
                 print (f'{fn} not found')
                 sys.exit(-1)
            params = joblib.load(fn)
            init_state = params['init_state']
            obs = params['obs']
            nominal_control=params['nominal_control'][0:self.max_path_length, :]
            self.irl_model.set_params(params['irl_params'])
            print ("Loading trained model succeeded!")
        else:
            obs = env.reset()
            init_state = self.get_env_state(env)
            nominal_control = self.policy.org_control
            sess.run(tf.global_variables_initializer())
        self.sampler.eval(itr, env, init_state, obs, nominal_control, num_steps, is_training=is_training, with_true_weights=with_true_weights, discount=self.discount, num_trials=num_trials)

    def load_latest_experts(self):
        if self.env.type_status == "CarRacing":
            max_expert_length = 500
            experts = []
            for i in range(self.num_demos):
                print(f"loading expert data {i} from {self.expert_datadir}")
                with open(f'{self.expert_datadir}/expert_{i}.pickle', 'rb') as handle:
                    if "0.2" in self.expert_datadir:
                        experts += pickle.load(handle)
                    else:
                        experts += pickle.load(handle)['traj']
            # for debug use
            # experts = []
            # for i in range(120):
            #     traj={}
            #     traj['observations'] = np.zeros((max_expert_length, 27648))
            #     experts.append(traj)
            # print (f"number of experts: {len(experts)}")
            obs = self.env.reset()
            init_state = self.get_env_state()
            num_experts = len(experts)
            path_obs_shape = (num_experts, max_expert_length, 96, 96, 3) #96x96x3 = 27648
            path_obs = np.zeros(path_obs_shape)
            path_valids = np.zeros((num_experts, max_expert_length, 1), dtype=np.uint8)
            for idx, traj in enumerate(experts):
                # print ("traj", traj["observations"].shape)
                valids = len(traj["observations"])
                valids = min(valids, max_expert_length)
#                 print (valids)
                path_valids[idx, :valids, :] = 1
                for i in range(valids):
                    path_obs[idx, i, :] = traj["observations"][i,:].reshape([96, 96, 3])
                    # path_obs[idx, i, :] = cv2.resize((traj["observations"][i, :].reshape([96, 96, 3]).astype('uint8')), (48, 48))
                    # path_obs[idx, i, :] = traj["observations"][i, :].reshape([48, 48, 3])
#                 print ("path_valids", path_valids)
            self.expert_path_length = max_expert_length
            # self.irl_model.expert_trajs_extracted = (path_obs, path_valids)
            self.irl_model.max_path_length = self.max_path_length
            self.irl_model.expert_trajs = (path_obs, path_valids)
        else:
            fname = get_expert_fnames(self.expert_datadir, self.num_demos)
            obs = self.env.reset()
            init_state = self.get_env_state()

            max_path_length = self.max_path_length
            if hasattr(fname, '__iter__'):
                path_obs = []
                path_valids = []
                path_lens = []
                with tf.variable_scope("load_policy", reuse=tf.AUTO_REUSE) as scope:
                    fname = get_expert_fnames(self.expert_datadir, self.num_demos)
                    for fname_ in fname:
                        snapshot_dict = joblib.load(fname_)
                        for path in snapshot_dict['paths']:
                            path_lens.append(path['observations'].shape[0])
                    self.max_path_length = int(np.mean(path_lens))
                    self.expert_path_length = self.max_path_length
                    self.start_worker()
                    fname = get_expert_fnames(self.expert_datadir, self.num_demos)
                    for fname_ in fname:
                        snapshot_dict = joblib.load(fname_)
                        policy = snapshot_dict['policy']
                        obses, valids = self.sampler.fast_expert_sampling(init_state, obs, policy, len(snapshot_dict['paths']))
                        path_obs.append(obses)
                        path_valids.append(valids)
                    self.shutdown_worker()
                path_obs = np.concatenate(path_obs, axis=0)
                path_valids = np.concatenate(path_valids, axis=0)
            else:
                with tf.variable_scope("load_policy", reuse=tf.AUTO_REUSE) as scope:
                    snapshot_dict = joblib.load(fname)
                    policy = snapshot_dict['policy']
                    self.start_worker()
                    path_obs, path_valids = self.sampler.fast_expert_sampling(init_state, obs, policy, len(snapshot_dict['paths']))
                    self.shutdown_worker()
            self.irl_model.expert_trajs_extracted = (path_obs, path_valids)
            self.max_path_length = max_path_length
        return init_state, obs

    def train(self, sess):
        sess.run(tf.global_variables_initializer())
        if self.init_pol_params is not None:
            self.policy.set_param_values(self.init_pol_params)
        if self.init_irl_params is not None:
            self.irl_model.set_params(self.init_irl_params)

        start_time = time.time()

        # load experts
        init_state, obs = self.load_latest_experts()

        self.start_worker()
        print ("after starting worker")
        dummpy_obs = self.env.reset()
        print ("reset")
        self.set_env_state(init_state)
        print ("set env")
        org_obs = obs
        org_state = init_state
        step = 0
        print (self.start_itr, self.n_itr)
        for itr in range(self.start_itr, self.n_itr):
            itr_start_time = time.time()
            with logger.prefix('itr #%d | ' % itr):
#                 self.sampler.reset_to_state(init_state, obs)
#                 logger.log("Obtaining samples...")
#                 paths = self.obtain_samples(itr)
#                 rwd = self.log_avg_returns(paths)
#                 logger.log("Processing samples...")
#                 #samples_data = self.sampler.process_samples(itr, paths, with_weights=True)
#                 samples_data = self.process_samples(itr, paths)
#                 logger.log("Logging diagnostics...")
#                 self.log_diagnostics(paths)
#                 weights = self.irl_model.fit(samples_data, self.lr, self.discount)
#               test eval
# simulate the two loops baseline
#                 for i in range(20):
#                 new_control = self.sampler.fast_sampling_and_processing(itr, init_state, obs, is_training=False) + self.policy.nominal_control
#                 self.policy.update_nominal_control(new_control)
                new_control = self.sampler.fast_sampling_and_processing(itr, init_state, obs, is_training=True) + self.policy.nominal_control
                self.policy.update_nominal_control(new_control)
                self.__irl_params = self.irl_model.get_params()
                params = self.get_itr_snapshot(itr, org_state, org_obs)  # , **kwargs)
                #params = self.get_itr_snapshot(itr, init_state, obs)  # , **kwargs)
                action = self.policy.get_current_control()
                obs, reward, done, info = self.env.step(action)
                if done or step > self.expert_path_length*1.3 or not self.sampler.good_itr:
                    print ('reset trajectory')
                    # reset again
                    obs = self.env.reset()
                    self.set_env_state(org_state)
                    init_state = org_state
                    obs = org_obs
                    self.policy.update_nominal_control(self.policy.org_control)
                    step = -1
                else:
                    #update state
                    init_state = self.get_env_state()
                    self.policy.elapse_one_step(step)
                step += 1
                #if self.store_paths:
                #    params["paths"] = samples_data["paths"]
                logger.record_tabular('ItrTime', time.time() - itr_start_time)
                itr_eval_time = time.time()
                logger.log(f"Start evaluating {itr}...")
                #self.sampler.eval(itr, self.env, init_state, obs, self.policy.nominal_control, 100, False, 0.99)
                logger.save_itr_params(itr, params)
                logger.log(f"Saved Done: {done}")
                logger.record_tabular('ItrEvalTime', time.time() - itr_eval_time)
                logger.record_tabular('Time', time.time() - start_time)
                logger.dump_tabular(with_prefix=False)
                if self.plot:
                    self.update_plot()
                    if self.pause_for_plot:
                        input("Plotting evaluation run: Press Enter to "
                              "continue...")
        self.shutdown_worker()

    def full_eval(self):
        self.batch_size = self.eval_batch_size
        self.start_worker()
        for itr in range(self.start_itr, self.n_itr):
            with logger.prefix('itr #%d | ' % itr):
                self.itr_eval(itr)
        self.shutdown_worker()

    def log_diagnostics(self, paths):
        self.env.log_diagnostics(paths)
        self.policy.log_diagnostics(paths)

    def get_itr_snapshot(self, itr, init_state, obs):
        # print ("irl model params", self.get_irl_params())
        return dict(
            itr=itr,
            policy=self.policy,
            policy_params=self.policy.get_param_values(),
            irl_params=self.get_irl_params(),
            nominal_control=self.policy.hist_control,
            init_state=init_state,
            obs=obs
        )
